# -*- coding: utf-8 -*-
"""NLPAssign3a(DT,SVM,MLP).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GcQayR90yrGJzKhu_O7enpcfybuEI06m
"""

from google.colab import drive
drive.mount("/content/drive")

!pip install contractions

!pip install nltk==3.5

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold
import pickle
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from html.parser import HTMLParser
import contractions
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords
from textblob import TextBlob
import sys



# Cleaning text
def preprocessing(text):
  clean_5=[]
  html_parser = HTMLParser()
  for sent in text:
  # for sent in train_df['5']:
    clean_S = html_parser.unescape(sent)
    clean_S = re.sub(r"@(\s+|\w+)","",clean_S)  #users eliminate
    clean_S = re.sub(r"https?:\/\/.*", "", clean_S)   #urls eliminate
    clean_S = re.sub(r"www.(\w+\.\w+)+", "", clean_S)   #urls eliminate
    clean_S = contractions.fix(clean_S)   #expand
    clean_S = re.sub(r"[lL][eE][tT]'s","let us",clean_S)   #expand
    clean_S = re.sub(r"[cC][aA][nN]'[tT]","cannot",clean_S)  #expand
    clean_S = re.sub(r"[012456789]+","",clean_S)  #digit eliminate
    clean_S = re.sub(r"^[:|;|\-|']","",clean_S)
    clean_S = re.sub(r"\b[xX]+\b","",clean_S)
    #  clean_S = re.sub(r"(\s+|\w)[\-|\->|<\-]*(\s+|w)","",clean_S)
    #  clean_S = re.sub('[|\=~+^\$\,\."\(\)&%_]', '', clean_S)   #junk symbols eliminate
    clean_S = re.sub(r'\.+', '.', clean_S)
    clean_S = re.sub(r'\,+', ',', clean_S)
    clean_S = re.sub(r'\:+', ':', clean_S)
    clean_S = re.sub('[^a-zA-Z3!\?\s\'#:;\-*\\<>\/.,]', '', clean_S)  #to eliminate non ascii character
    clean_5.append(clean_S)
    #  print(clean_S)
  # for i in clean_5:
    # print(i)
  #print(len(clean_5))
  return clean_5

import nltk
nltk.download('punkt')

# tokens bnake contract +('\w{1,3}\b)
# feature 1 : all cap
def allCapFeature(cleanTextList,features):
  all_caps=[]
  for row in cleanTextList:
    words=word_tokenize(row)
    for word in words:
      c=0
      if (word.isupper()):
        c=c+1
    all_caps.append(c)
  # print(len(all_caps))
  # print(all_caps)
  features['all_caps_count']=all_caps
  return features

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

# POS feature
# feature 2 :postag feature
def postagFeature(cleanTextList,flag,features):
    f_pos=[]  #list to store feature
    # Number of occurence of each pos tag(incomplete).
    # clean_aux = re.sub(r"[^a-zA-Z]","",clean_5) 
    for row in cleanTextList:
      # print(row)
      correct_spell=TextBlob(row).correct()
      correct_s = str(correct_spell)
      # print(correct_s)
      word_tokens=nltk.word_tokenize(correct_s)
      # print(word_tokens)
      word_nostopw=[]
      for word in word_tokens:
        if word not in stopwords.words('english'):
          word_nostopw.append(word)
      # print(word_nostopw)    
      pos_t=nltk.pos_tag(word_nostopw)
      # print(pos_t)
      pos_ut=""
      for i in range(len(pos_t)):
        if(len(pos_t[i][1])>1):
          pos_ut = pos_ut+" "+pos_t[i][1].lower()
      f_pos.append(pos_ut)  
    # print(f_pos)
    from sklearn.feature_extraction.text import CountVectorizer
    import pickle
    from sklearn.feature_extraction.text import CountVectorizer
    pkl_file1="/content/drive/My Drive/assignment3a/vectorizer_pos_tag.pkl"
    if(flag==0):
      vectorizer = CountVectorizer()
      X = vectorizer.fit_transform(f_pos)
      pos_feature=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())
      with open(pkl_file1, 'wb') as file:
          pickle.dump(vectorizer, file)
    else:
        with open(pkl_file1, 'rb') as file:
            vectorizer= pickle.load(file)
        X=vectorizer.transform(f_pos)
        pos_feature=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())
  
    features=pd.concat([features,pos_feature], axis=1)  
    return features

# Hashtag feature
def hashtagFeature(cleanTextList,features):
    hashtags=[]
    for row in cleanTextList:
      # print(row)
      ll=re.findall(r"#\w+",row)
      hashtags.append(len(ll))
    features['hashtag']=hashtags  
    #print(hashtags)
    return features

# Punctuation feature
def punctuationFeature(cleanTextList,features): 
  end_mark=[]
  punc_occ=[]
  # 6 rows cleaning error
  for row in cleanTextList:
    ll=re.findall(r"(([!|?]{2,})|[!|?]\s+[!|?])+",row)
    ll1=re.findall(r"[!|?]$",row)
    end_mark.append(len(ll1))
    punc_occ.append(len(ll))
  # print(punc_occ)  
  # print(end_mark)
  features['punc_occurance']=punc_occ
  features['end_mark']=end_mark
  return features

# elongated words
def elongationReplacer(cleanTweetList,features):  
  elong=[]
  for row in cleanTweetList:
          # print(row)
          row=re.sub(r"[^a-zA-Z]","",row)
          regex = re.compile(r"(.)\1{2}")
          ll=len([word for word in row.split() if regex.search(word)])
          # print(ll)
          elong.append(ll)
  # print(elong)     
  features['elongation_counter']=elong
  return features

!pip install afinn
!pip install emot

from afinn import Afinn
import emot

# Emoticon feature
# f_emot=[]
# for row in clean_5:
  # ll=re.findall(':\)|;\)|:-\)|;-\)|\(:|\(-:|:-D|:D|=D|:P|:-P|[xX]D|[xX]-p|\^\^|:-\*|:\*|\^\.\^|\^\-\^|\^\_\^|\,-\)|,\)|\)-:|:\'\(|:\(|:-\(|:\S|T\.T|\.\_\.|:<|:-\S|:-<|\*\-\*|:O|=O|=\-O|O\.o|XO|O\_O|:-\@|=\/|:\/|X\-\(|>\.<|>=\(', row)
  # f_emot.append(len(ll))
# print(len(f_emot))
# from afinn import Afinn
def isEmoticon(word):
    x=emot.emoticons(word)
    
    if isinstance(x, list):
        for v in x:
            val=v['flag']
    else:
        val=x['flag']
    
    if(val):
        return True
    else:
        return False

def emoticonScore(cleanTextList,features):
    n1score=[]
    p1score=[]
    pp1=False
    pn1=False
    lastword=[]
    afinn=Afinn(emoticons=True)
    for row in cleanTextList:
       tweets=row.split()
       #print(tweets)
      #  print(tweets)
       if(len(tweets)):
          t=tweets[-1]
          lastword.append(t)
       else: 
          lastword.append("")   
       for tweet in tweets:
         if(isEmoticon(tweet)==True):  
            score=afinn.score(tweet)
            if score>0:
                pp1=True
            elif score<0:
                pn1=True
       if (pp1==True):
         p1score.append(1)
       else:
         p1score.append(0)
       if(pn1==True):
         n1score.append(1)
       else:
         n1score.append(0) 
    last_p=[]
    last_n=[]
    for word in lastword:
      if(isEmoticon(word)):
        score=afinn.score(word)
        if score>0:
          last_p.append(1)
          last_n.append(0)
        elif score<0:
           last_p.append(0)
           last_n.append(1)
        else:
          last_p.append(0)
          last_n.append(0)
      else:
        last_p.append(0)
        last_n.append(0)  
    features['pos_emoticon_presence']=p1score
    features['neg_emoticon_presence']=n1score
    features['pos_emoticon_presence_last']=last_p
    features['neg_emoticon_presence_last']=last_n
    return features
    # features['pos_emoticon_score']=p1score
    # features['neg_emoticon_score']=n1score
    # return features
# for row in clean_5[:20]:
    # cleanTweet=row.split()

# sentiment140
sent140Dict = {}

with open('/content/drive/My Drive/lexicons.zip (Unzipped Files)/lexicons/3. Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt', 'r') as f:
    for row in f.readlines():
        row = row.split()
        sent140Dict[row[0]] = float(row[1])
print(sent140Dict)

# Hashtsent
hastag_sentDict={}
with open('/content/drive/My Drive/lexicons.zip (Unzipped Files)/lexicons/7. NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt', 'r') as f:
    for row in f.readlines():
        row = row.split()
        hastag_sentDict[row[0]] = float(row[1])
print(hastag_sentDict)

NRCwordDict = {}
# emotionList = {}
with open('/content/drive/My Drive/lexicons.zip (Unzipped Files)/lexicons/8. NRC-word-emotion-lexicon.txt', 'r') as f:
    for row in f.readlines():
        row = row.split()
        if (row[1]=="positive" or row[1]=="negative"):
            NRCwordDict[row[0]+"_"+row[1]] = float(row[2])

print(NRCwordDict)

# Lexicon feature
def lexiconFeature(cleanTextList,features):
  Nrc_tot_con_greaterthan0=[]
  Nrc_tot_score=[]
  Nrc_last_score=[]
  hastag_tot_con_greaterthan0=[]
  hastag_tot_score=[]
  hastag_last_score=[]
  sent140_tot_con_greaterthan0=[]
  sent140_tot_score=[]
  sent140_last_score=[]
  for row in cleanTextList:
      # print(row)
      row = re.sub(r"[^a-zA-Z]$","",row) 
      word_tokens=word_tokenize(row)
      word_tokens=[word.lower() for word in word_tokens]
      # print(word_tokens) 
      count_great_0=0
      total_score=0
      last_soce=0
      for word in word_tokens:
        if(word in sent140Dict): 
            sc=sent140Dict.get(word)
            total_score=total_score+sc
            if(sc>0):
              count_great_0 = count_great_0 + 1
      if(len(word_tokens)>0):         
        if(word_tokens[len(word_tokens)-1] in sent140Dict):
          sent140_last_score.append(sent140Dict.get(word_tokens[len(word_tokens)-1]))
        else:
          sent140_last_score.append(0)  
      else:
        sent140_last_score.append(0)           
      sent140_tot_con_greaterthan0.append(count_great_0)
      sent140_tot_score.append(total_score)
      count_great_0=0
      total_score=0
      last_soce=0
      for word in word_tokens:
        if(word in hastag_sentDict): 
            sc=hastag_sentDict.get(word)
            total_score=total_score+sc
            if(sc>0):
              count_great_0 = count_great_0 + 1
      if(len(word_tokens)>0):         
        if(word_tokens[len(word_tokens)-1] in hastag_sentDict):
          hastag_last_score.append(hastag_sentDict.get(word_tokens[len(word_tokens)-1]))
        else:
          hastag_last_score.append(0)  
      else:
        hastag_last_score.append(0)         
      hastag_tot_con_greaterthan0.append(count_great_0)
      hastag_tot_score.append(total_score)
      count_great_0=0
      total_score=0
      last_soce=0
      for word in word_tokens:
        if(word+"_positive"  in NRCwordDict): 
            sc=NRCwordDict.get(word+"_positive")
            total_score=total_score+sc
            if(sc>0):
              count_great_0 = count_great_0 + 1
        if(word+"_negative"  in NRCwordDict): 
            sc=NRCwordDict.get(word+"_negative")
            total_score=total_score+sc
            if(sc>0):
              count_great_0 = count_great_0 + 1       
      Nrc_tot_con_greaterthan0.append(count_great_0)
      Nrc_tot_score.append(total_score)
      if(len(word_tokens)>0):
        if(word_tokens[len(word_tokens)-1]+"_positive" in NRCwordDict):
          Nrc_last_score.append(NRCwordDict.get(word_tokens[len(word_tokens)-1]+"_positive"))
        elif(word_tokens[len(word_tokens)-1]+"_negative" in NRCwordDict):
          Nrc_last_score.append(NRCwordDict.get(word_tokens[len(word_tokens)-1]+"_negative"))
        else:
          Nrc_last_score.append(0)    
      else:
        Nrc_last_score.append(0) 
  features['NRC lexicon greater than 0']=Nrc_tot_con_greaterthan0
  features['NRC lexicon total score']=Nrc_tot_score
  features['NRC lexicon last token']=Nrc_last_score
  features['Hashtag Sentiment lexicon greater than 0']=hastag_tot_con_greaterthan0
  features['Hashtag Sentiment lexicon total score']=hastag_tot_score
  features['Hashtag Sentiment lexicon last token']=hastag_last_score
  features['Sentiment140 lexicon greater than 0']=sent140_tot_con_greaterthan0
  features['Sentiment140 lexicon total score']=sent140_tot_score
  features['Sentiment140 lexicon last token']=sent140_last_score
  return features

  # if (len(word_tokens)>0):
      # sc=lexDict.get(word_tokens[len(word_tokens)-1])
    # else:  
      # sc=0





# Negated Feature
def negationCount(cleanTextList,features):
  from nltk.sentiment.vader import VaderConstants
  neg_count=[]
  for row in cleanTextList:
      # print(row)
      # print(last_punc)
      # row = re.sub(r'[^a-zA-Z\s]','',row)
      correct_spell=TextBlob(row).correct()
      correct_s = str(correct_spell)
      # print(correct_s)
      word_tokens=nltk.word_tokenize(correct_s)
      # print(word_tokens)
      negct=0
      sid_obj = VaderConstants()
      if(len(word_tokens)>0):
        last_punc=False
        if(word_tokens[len(word_tokens)-1]=='.' or word_tokens[len(word_tokens)-1]==',' or word_tokens[len(word_tokens)-1]==':'or word_tokens[len(word_tokens)-1]==';' or word_tokens[len(word_tokens)-1]=='!' or word_tokens[len(word_tokens)-1]=='?' ):
          last_punc=True
      for word in word_tokens:
          fs=sid_obj.negated([word],include_nt=True)
          #  print(fs)
          if(fs==True and last_punc==True):
            negct = negct+1
      neg_count.append(negct)
      # print(negct)
  #print(neg_count)
  features['negation_count']=neg_count       
  return features



import nltk
nltk.download('wordnet')

import nltk
nltk.download('punkt')
nltk.download('stopwords')

def lemmatization(cleanTextList):
  lm=WordNetLemmatizer()
  u_bag=[]
  for row in cleanTextList:
      row = re.sub(r'[^a-zA-Z\s]','',row)
      correct_spell=TextBlob(row).correct()
      correct_s = str(correct_spell)
      # print(correct_s)
      word_tokens=nltk.word_tokenize(correct_s)
      # print(word_tokens)
      word_nostopw=""
      for word in word_tokens:
        word= lm.lemmatize(word)
        if word not in stopwords.words('english'):
            word_nostopw = word_nostopw+" "+word
      f_str = word_nostopw
      u_bag.append(f_str)
  return u_bag



def ngram(cleanTextList,flag,no,name):
  from sklearn.feature_extraction.text import CountVectorizer
  import pickle
  from sklearn.feature_extraction.text import CountVectorizer
  pkl_file1="/content/drive/My Drive/assignment3a/"+name
  if(flag==0):
     vectorizer = CountVectorizer(ngram_range=(no,no))
     X1=vectorizer.fit_transform(cleanTextList)
     res=pd.DataFrame(X1.toarray(),columns=vectorizer.get_feature_names())
     with open(pkl_file1, 'wb') as file:
            pickle.dump(vectorizer, file) 
  else:
    with open(pkl_file1, 'rb') as file:
              vectorizer= pickle.load(file)
    X1=vectorizer.transform(cleanTextist)
    res=pd.DataFrame(X1.toarray(),columns=vectorizer.get_feature_names()) 
  return res

# Non contigous feature
def contiguous(cleanTextList):
  u_bag=[]
  for row in cleanTextList:
      row = re.sub(r'[^a-zA-Z\s]','',row)
      correct_spell=TextBlob(row).correct()
      correct_s = str(correct_spell)
      # print(correct_s)
      word_tokens=nltk.word_tokenize(correct_s)
      # print(word_tokens)
      word_nostopwe=""
      word_nostopwo=""
      for i in range(len(word_tokens)):
        word= lm.lemmatize(word_tokens[i])
        if word not in stopwords.words('english'):
            if(i%2==0):
            word_nostopwe = word_nostopwe+" "+word
            else
      f_str = word_nostopw
      u_bag.append(f_str)
  #print(u_bag)
  return u_bag

#preprocessing and feature extraction of train file
train_df=pd.read_csv('/content/drive/My Drive/sentiment_train.csv')
#test_df=pd.read_csv('/content/drive/My Drive/sentiment_test.csv')
cleanTextList=preprocessing(train_df['5'][:6000])    
features=pd.DataFrame()
features=allCapFeature(cleanTextList,features)
features=postagFeature(cleanTextList,0,features)
print(features)
features=hashtagFeature(cleanTextList,features)
print(features)    
features=punctuationFeature(cleanTextList,features)         
features=elongationReplacer(cleanTextList,features)
print(features)
features=emoticonScore(cleanTextList,features)
print(features)
features=lexiconFeature(cleanTextList,features)
print(features)
features.to_pickle("/content/drive/My Drive/assignment3a/train_features_till_lexicon.pkl")
train_features=pd.read_pickle("/content/drive/My Drive/assignment3a/train_features_till_lexicon.pkl")
train_features=negationCount(cleanTextList,train_features)
print(train_features)
train_features.to_pickle("/content/drive/My Drive/assignment3a/train_features_till_negation.pkl")

lemmatizedlist=lemmatization(cleanTextList)  
import pandas as pd
lemmatizer_df=pd.DataFrame()
lemmatizer_df['list']=lemmatizedlist
lemmatizer_df.to_pickle("/content/drive/My Drive/assignment3a/train_lemmatizedText.pkl")

import pandas as pd
import pickle
l_df=pd.read_pickle("/content/drive/My Drive/assignment3a/train_lemmatizedText.pkl")
lemmatizedlist=l_df['list']
unigram=ngram(lemmatizedlist,0,1,'vectorizer_1.pkl')
bigram=ngram(lemmatizedlist,0,2,'vectorizer_2.pkl')
  #trigram=ngram(lemmatizedlist,0,3,'vectorizer_3.pkl')
train_features=pd.concat([train_features,unigram,bigram],axis=1)
 
train_features.to_pickle("/content/drive/My Drive/assignment3a/train_features.pkl")

#feature extraction and preprocesisng of the test file.
test_df=pd.read_csv('/content/drive/My Drive/sentiment_test.csv')
cleanTextList_test=preprocessing(test_df['5'][:6000])    
features_test=pd.DataFrame()
features_test=allCapFeature(cleanTextList_test,features_test)

features_test=postagFeature(cleanTextList_test,1,features_test)
print(features_test)
features_test=hashtagFeature(cleanTextList_test,features_test)
print(features_test)    
features_test=punctuationFeature(cleanTextList_test,features_test)        
features_test=elongationReplacer(cleanTextList_test,features_test)
print(features_test)
features_test=emoticonScore(cleanTextList_test,features_test)
print(features_test)
features_test=lexiconFeature(cleanTextList_test,features_test)
print(features_test)
test_features=negationCount(cleanTextList_test,test_features)
print(test_features)
lemmatizedlist=lemmatization(cleanTextList_test)  
lemmatizer_df=pd.DataFrame()
lemmatizer_df['list']=lemmatizedlist
lemmatizer_df.to_pickle("/content/drive/My Drive/assignment3a/test_lemmatizedText.pkl")
unigram=ngram(lemmatizedlist,1,1,'vectorizer_1.pkl')

unigram.to_pickle("/content/drive/My Drive/assignment3a/unigram_test_feature.pkl")
bigram=ngram(lemmatizedlist,1,2,'vectorizer_2.pkl')
bigram.to_pickle("/content/drive/My Drive/assignment3a/bigram_test_feature.pkl")

test_features=pd.concat([test_features,unigram,bigram],axis=1)
print(test_features)

test_features.to_pickle("/content/drive/My Drive/assignment3a/test_final_feature.pkl")



#model
def decisiontree(x_train,y_train,pkl_file):
    from sklearn.tree import DecisionTreeClassifier
    from sklearn import metrics
    import pickle
    
    # DT Classifier
    dt=DecisionTreeClassifier()
    dt.fit(x_train, y_train)
    with open(pkl_file, 'wb') as file:
          pickle.dump(dt, file)

import pandas as pd
train_features=pd.read_pickle("/content/drive/My Drive/assignment3a/train_features.pkl")

#fitting the model
file='/content/drive/My Drive/assignment3a/dtClassifiermodel.pkl'
decisiontree(train_features,train_df['0'][:6000],file)

def svmClassifier(x_train,y_train,pkl_file):
  from sklearn import svm
  model=svm.SVC(kernel='linear')
  model.fit(x_train, y_train)
  with open(pkl_file, 'wb') as file:
          pickle.dump(model, file)

#fitting the model
file1='/content/drive/My Drive/assignment3a/SVMClassifiermodel.pkl'
svmClassifier(train_features,train_df['0'][:6000],file1)

def MLP(x_train,y_train,pkl_file):
  from sklearn.neural_network import MLPClassifier
  import pickle
  model=MLPClassifier()
  model.fit(x_train, y_train)
  with open(pkl_file, 'wb') as file:
          pickle.dump(model, file)

#fitting the model
file2='/content/drive/My Drive/assignment3a/MLPClassifiermodel.pkl'
MLP(train_features,train_df['0'][:6000],file2)







#testing the fitted model over the 6000 rows.
test_features=pd.read_pickle("/content/drive/My Drive/assignment3a/test_final_feature.pkl")

#function to compute the accuracy , precision, recall , F1 score and confusion matrix and saving in a file.
def calculateparameter(ExpectedTags,PredictedTags,file):

    l1=list(dict.fromkeys(ExpectedTags))
    l2=list(dict.fromkeys(PredictedTags))    
    #pairs=nltk.FreqDist(pair)

    df=pd.DataFrame(0, columns=l1, index=l2)
    for i in range(0,len(ExpectedTags)):
        df.loc[PredictedTags[i],ExpectedTags[i]]+=1
    #print(df)
    file=open(file,'a+')
    #.savetxt('output.txt', df.values, fmt='%d', delimiter="\t")
    df.to_csv(file, header=None, index=None, sep=' ', mode='a')  
    #TP values:
    file.write("\ntagWisePrecision\n")
    df['rowSum']=df.sum(axis=1)
    precision=[]
    recall=[]
    row=[]
    for col in df.columns:
        if col != 'rowSum':
            if col in df.index:
                value=df.loc[col,col]
                precision.append(value/df.loc[col,'rowSum'])
                row.append(col)
                file.write(str(col)+" "+str(value/df.loc[col,'rowSum'])+"\n")
            else:
                value=0
           

    
    total=0
    for i in range(0,len(precision)):
        total+=precision[i]
    r1=total/len(precision)    
    file.write("\nmacro averaged Precision: "+str(total/len(precision))+"\n")    

    file.write("\ntagWiseRecall:\n")
    df.loc['colSum']=df.sum(axis = 0)
    for col in df.columns:
        if col != 'rowSum':
            if col in df.index:
                recall.append(df.loc[col,col]/df.loc['colSum',col])
                file.write(str(col)+" "+str(df.loc[col,col]/df.loc['colSum',col])+"\n")  
            else:
                #recall.append(0)
                file.write(str(col)+" "+str(0)+"\n")
    total=0
    for i in range(0,len(recall)):
        total+=recall[i]    
    r2=total/len(recall)    
    file.write("\nmacro averaged recall: "+str(total/len(recall))+"\n")

    f1score=[]
    file.write("\nF1 score tag wise:")
    for i in range(0,len(recall)):
        if (recall[i]==0 and precision[i] == 0):
            value=0
        else:
            value=(2*recall[i]*precision[i])/(recall[i]+precision[i])
        file.write(str(row[i])+" "+str(value)+"\n")
        f1score.append(value)
    total=0
    for i in range(0,len(f1score)):
        total+=f1score[i]  
    r3=total/len(f1score)    
    file.write("\nmacro averaged f1score: "+str(total/len(f1score))+"\n\n\n")
    value=0
    for col in df.columns:
        if col != 'rowSum':
            if col in df.index:
                value+=df.loc[col,col]
            else:
                value+=0
    file.write("\nAccuracy: "+str(value/df.loc['colSum','rowSum'])+"\n\n\n")  
    r4=value/df.loc['colSum','rowSum']
    return r1, r2,r3,r4



#predicting using decision tree classifier.
import pickle
with open('/content/drive/My Drive/assignment3a/dtClassifiermodel.pkl', 'rb') as file:
    dt_pickle_model = pickle.load(file)
y_predict_dt=dt_pickle_model.predict(test_features)

ExpectedTags = test_df['0'][:6000]
PredictedTags = y_predict_dt
r1,r2,r3,r4=calculateparameter(ExpectedTags,PredictedTags,'/content/drive/My Drive/assignment3a/dt_output.txt')
print("Macro averaged precision : "+str(r1))
print("Macro averaged recall : "+str(r2))
print("Macro averaged f1 score : "+str(r3))
print("accuracy: "+str(r4))

#predicting using mlp
import pickle
with open('/content/drive/My Drive/assignment3a/MLPClassifiermodel.pkl', 'rb') as file:
    mlp_pickle_model = pickle.load(file)
y_predict_mlp=mlp_pickle_model.predict(test_features)

ExpectedTags = test_df['0'][:6000]
PredictedTags = y_predict_mlp
r1,r2,r3,r4=calculateparameter(ExpectedTags,PredictedTags,'/content/drive/My Drive/assignment3a/mlp_output.txt')
print("Macro averaged precision : "+str(r1))
print("Macro averaged recall : "+str(r2))
print("Macro averaged f1 score : "+str(r3))
print("accuracy: "+str(r4))

#prediction using  svm
with open('/content/drive/My Drive/assignment3a/SVMClassifiermodel.pkl', 'rb') as file:
    svm_pickle_model = pickle.load(file)
x_test=test_features
y_predict=svm_pickle_model.predict(x_test)

ExpectedTags = test_df['0'][:6000]
PredictedTags = y_predict
r1,r2,r3,r4=calculateparameter(ExpectedTags,PredictedTags,'/content/drive/My Drive/assignment3a/svm_output.txt')
print("Macro averaged precision : "+str(r1))
print("Macro averaged recall : "+str(r2))
print("Macro averaged f1 score : "+str(r3))
print("accuracy: "+str(r4))

